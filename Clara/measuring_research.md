## Measuring Research: What everyone needs to know

#### Authors: Cassidy Sugimoto, Vincent Larivière
#### Date of Publlication: 2018

#### *Citation*
Sugimoto, C. R., & Larivière, V. (2018). Measuring research: What everyone needs to know. Oxford University Press.


#### Descriptive Summary
Cassidy and Vincent intend this book to be a gentle introduction to the uses, context, and historical foundations of bibliometrics insofar as they will be relevant and helpful to researchers as they encounter bibliometric ranking and studies of their own work and field. Since there has been an expansion of the uses and importance of bibliometrics and scientometrics as a means of evaluation and as a practice for getting a stronger understanding of the shape and context of ones' own field for general use purposes, they felt this volume was needed to give background and context for people who will need to use and interact with bibliometrics, but who do not yet know this context. 

Essentially the overall thesis is that when working with bibliometrics databases, indices and practices, researchers need to understand the way that the data was gathered--which may have some incomplete coverage--the way the indices were developed and what exactly they measure--which creates certain limitations to their broad applicability, as they tend to measure something super-specific as a proxy for the Understanding Of Science or faculty ranking or whatever. This is a feature of all data (re. Raw Data is an Oxymoron?), but the uses of bibliometrics as an evaluation tool can give great stakes to not understanding all these nuances. Many of the quirks of modern bibliometric tools are informed by their history and initial development as aggregation tools for libraries. Cassidy and Vincent describe these origins and their artefacts in the current system. Other quirks are effects of the (very often corporate) organizations that own, control and curate them. Cassidy and Vincent also discuss this and offer some reflection that maybe it's not so great that science metrics and the vast majority of scientific journals are controlled by corporate interests who scientists give their research to and data (via article metadata), but then have to pay to access. 
Basically they argue in favor of greater understanding of context, deliberateness and thoughfulness when measuring research. Know the strengths and weaknesses--the coverage--of the database you use, know the strengths and weakesses of the measures etc. Know that the results of the measurement aren't everthing. 

1. The Basics:

a lot about what the book is for and who it's for. useful
Discussion of the development of the field--different directions it developed from with a big tent of different types of profesionals and researchers interested in it for differnt reasons:
* earliest, early 20th c--Statistical bibliography conducted by librarians to aid in their decision making regarding aquisitions 
* also early 20th c--governments wanting indicators of scientific development--later in 1950, 1960ish tied up with "institutionalization of scientific research" needing indicators to evaluate and apportion funding
* 1960ish Derek De Solla Price
* as the field developed, it splits into STS and Scientometrics (constructivist and positivist respectively)

Strong theoretical foundations:
* Lotke's law
* Matthew's law--> cumulative advantage and mathilda effect (cumulative disadvantage for women)
* 1990s, Network science enters: powerlaw and cumulative advantage ^^^

briefly goes over what indicators are, and goes over major databases

Different databases will have better coverage for certain fields, or types of scientific reserach. 

4. The Big Picture

delves into and critiques the dynamic that corporate interests control vast majority of journals, and most of the bibliometrics databases and tools. They also make their own indicators. Some things to keep in mind.
What does it mean for science that corporate interests control much of the infrastructure that allows science to work? 

Some tools that are so widely used are useful BECAUSE so many use them. however the ill effect is that too often the people who use and refence these indicators are not aware of the limitations of these measures--discuses H index as an example of this--widely disliked by scientometric community, widely used anyway (not least because it's what GoogleScholar uses)  We can't use these things naively

Also, when private interests control these databases, they may change the structures and access to the databases to best suit their own needs. e.g. Elsevier did this (121)

In the end, they offer a neat checklist of things for stakeholders to keep in mind: 
"Although there are nuances to the responsibilities of each stakeholder group, there are five key issues that all stakeholder groups should keep in mind: (1) time, (2) data quality, (3) normalization, (4) coverage, and (5) alignment."(125)

#### Relevant (analytical/critical) notes
Not much that is new to me at this point, but very useful to have many of the major points, historical background and debates tied up in one place. Chapter 4 was most interesting 

#### Quotes

Bibliometric data can provide great insight into the scientific system, but must be understood within its unique context. This requires not only an understanding of the technical and mathematical properties of the data, but also of the larger sociological and economic processes that govern the science system.

Sugimoto, Cassidy R.. Measuring Research (What Everyone Needs To Know®) (p. 6). Oxford University Press. Kindle Edition. 

Second, throughout its history, bibliometrics has attracted contributions from scholars coming from a large array of disciplines. While most of the first bibliometricians were natural scientists aiming at understanding the structure of science, the method also attracted scientists interested in providing descriptive accounts of their own disciplines. This led to macro-level analyses as well as iconographic histories of science, focusing on the success of key men and events in science. Sociologists introduced a more theoretical approach, applying social theories to the scientific system. Physicists brought network science and economists brought economic models to the mix.

Sugimoto, Cassidy R.. Measuring Research (What Everyone Needs To Know®) (p. 7). Oxford University Press. Kindle Edition. 

The scientific community produces, but does not control, the data that govern it, and decisions about these data are often driven by corporate, rather than scientific, concerns.

Sugimoto, Cassidy R.. Measuring Research (What Everyone Needs To Know®) (p. 119). Oxford University Press. Kindle Edition. 

The h-index—although derided by the bibliometric community—was included in all major platforms because of the coverage it received within the research community and the subsequent demand by researchers and administrators who, in many cases, are unaware of the weaknesses of the indicator.

Sugimoto, Cassidy R.. Measuring Research (What Everyone Needs To Know®) (p. 120). Oxford University Press. Kindle Edition. 